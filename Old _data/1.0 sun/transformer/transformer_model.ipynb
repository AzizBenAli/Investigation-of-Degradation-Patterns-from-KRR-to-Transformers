{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n",
    "Training a transformer model to forecast time series sequence of stock closing price  \n",
    "Using 10 timesteps to forecast 1 forward timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benal\\AppData\\Local\\Temp\\ipykernel_17608\\1405314298.py:8: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.signal import savgol_filter\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils as torch_utils\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.decomposition import KernelPCA\n",
    " \n",
    "import pandas as pd \n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# %%\n",
    "# INDOORS 1 SUN\n",
    "file = pd.ExcelFile('Indoor MPP_1.0 sun_Air_ICN2_Feb. 2023_Tripple cat.-Variable Annealing temps.xlsx', engine='openpyxl')# the whole excel file \n",
    "sn = file.sheet_names # sheets that are the excel file \n",
    "data_frame = pd.read_excel(file,sn[0],  engine='openpyxl') # read the data from the first sheet \n",
    "\n",
    "temperature = data_frame.loc[(data_frame[\"Parameter\"]     ==\"Annealing temperature\")].to_numpy() #take the temperature from the row Annealing temperature of column Parameters and convert it to numpy\n",
    "temperature = temperature[2,2:] # take everything from temperature starting from the third row and the third column\n",
    " \n",
    "devices = data_frame.columns.to_numpy() # take the devices from the data and convert them to numpy\n",
    " \n",
    "devices = np.reshape(devices, (devices.shape[0],1)) \n",
    "devices = devices[2:] #excluding the first two elements\n",
    "\n",
    "time_series_df = pd.read_excel(file,sn[-1],  engine='openpyxl') # read the data from the last sheet\n",
    "\n",
    "time_series_df = time_series_df.drop(index =[0]) # drop the first row (einheiten)\n",
    "time = time_series_df.iloc[:,0] # take the time from the first column\n",
    "\n",
    "\n",
    "\n",
    "# print(time)\n",
    "time_series_1_sun = []\n",
    "for i in range (temperature.shape[0]):\n",
    "    time_series_1_sun.append(time_series_df.iloc[:,2+i*3]) ### for every device take the impp (optimal current output that the solar panel can generate when it is operating at its maximum power point)\n",
    "\n",
    "time_series_1_sun = np.array(time_series_1_sun) # convert it to array [shape(24, 4001)]\n",
    "\n",
    "\n",
    "# %% \n",
    "# 1.4 SUN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "# OUTDOOR CONDITIONS\n",
    "# file = pd.read_csv('outdoor_conditions.txt')\n",
    "outdoor_conditions = pd.read_csv('outdoor_conditions_unnormalized.txt', sep='\\t')\n",
    "\n",
    "\n",
    "# Save the normalized DataFrame to a new file\n",
    "# %%\n",
    "# OUTDOOR PANELS\n",
    "# file = pd.read_csv('outdoor_conditions.txt')\n",
    "outdoor_panels = pd.read_csv('outdoor_data_imputed.txt', sep ='\\t')\n",
    "# outdoor_panels.head()\n",
    "outdoor_panels_power = [] \n",
    "for i in range(temperature.shape[0]):\n",
    "    outdoor_panels_power.append(outdoor_panels[str(i)])\n",
    "outdoor_panels_power = np.array(outdoor_panels_power) #[shape(24, 4794)]\n",
    "\n",
    "#%%\n",
    "# Padding\n",
    "\n",
    "\n",
    "time_series_1_sun_shape = time_series_1_sun.shape\n",
    " \n",
    "time_series_1_sun = np.pad(time_series_1_sun, ((0,0),(0,outdoor_panels_power.shape[1] - time_series_1_sun.shape[1])),mode='edge')    \n",
    "    \n",
    "time_series_1_sun = np.reshape(time_series_1_sun, ((time_series_1_sun.shape[0],time_series_1_sun.shape[1],1)))\n",
    " \n",
    "outdoor_panels_power = np.reshape(outdoor_panels_power, ((outdoor_panels_power.shape[0],outdoor_panels_power.shape[1],1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.26533 56.629   18.8651 ]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average values for time_series_1_sun\n",
    "time_series_1_sun_average = np.zeros((12, time_series_1_sun.shape[1], 1))\n",
    "j = 0\n",
    "for i in range(time_series_1_sun.shape[0]):\n",
    "    if (i+1) % 2 == 0:\n",
    "        time_series_1_sun_average[j] = np.mean(time_series_1_sun[i-1:i+1], axis=0)\n",
    "        j += 1\n",
    " \n",
    "     \n",
    "\n",
    "# Calculate average values for time_series_1_4_sun\n",
    "\n",
    "\n",
    "# Calculate average values for outdoor_panels_power\n",
    "outdoor_panels_power_average = np.zeros((12, 4794, 1))\n",
    "b = 0\n",
    "for i in range(outdoor_panels_power.shape[0]):\n",
    "    if (i+1) % 2 == 0:\n",
    "        outdoor_panels_power_average[b] = np.mean(outdoor_panels_power[i-1:i+1], axis=0)\n",
    "        outdoor_panels_power_average[b][outdoor_panels_power_average[b] < 0] = 0 # Set negative values to 0\n",
    "        b += 1\n",
    "\n",
    "# Stack the outdoor conditions\n",
    "outdoor_conditions_stacked = np.stack([outdoor_conditions ] * 12, axis=0)\n",
    "outdoor_conditions_stacked = outdoor_conditions_stacked[:,:,2:]\n",
    "\n",
    "print(outdoor_conditions_stacked[0,0,:])\n",
    "# outdoor_conditions_stacked_2= np.stack([outdoor_conditions] * 20, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4794, 4)\n",
      "(12, 4794, 1)\n"
     ]
    }
   ],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    smoothed_series = [series[0]]  # Initialize the smoothed series with the first value of the original series\n",
    "    for i in range(1, len(series)):\n",
    "        smoothed_value = alpha * series[i] + (1 - alpha) * smoothed_series[-1]\n",
    "        smoothed_series.append(smoothed_value)\n",
    "    return np.array(smoothed_series)\n",
    " \n",
    "alpha = 0.1   \n",
    "for i in range (time_series_1_sun_average.shape[0]):\n",
    " \n",
    "  time_series_1_sun_average[i] = exponential_smoothing(time_series_1_sun_average[i], 0.1)\n",
    "  outdoor_panels_power_average[i] = exponential_smoothing(outdoor_panels_power_average[i], 0.7)\n",
    "all_new_features = np.concatenate((time_series_1_sun_average , outdoor_conditions_stacked), axis=2)\n",
    "print(all_new_features.shape)\n",
    "print(outdoor_panels_power_average.shape)\n",
    "\n",
    "\n",
    "#TODO REMOVE PADDING SEE IF IT BREAKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU for computations.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available. Using GPU for computations.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA is not available. Using CPU for computations.')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot shows the reduced scale of the closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=4, num_layers=1, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=2, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)  # TODO ADD A BETTER NN WITH ACTIVATIONS AND STUFF\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src) or not torch.is_tensor(self.src_mask):\n",
    "#             device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "#             mask= torch.from_numpy(mask)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\benal\\indoor-outdoor_stability_prediction\\Old _data\\1.0 sun\\transformer\\transformer_model.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()  \u001b[39m# Set the model in training mode\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mview(X_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m4794\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, y_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\benal\\indoor-outdoor_stability_prediction\\Old _data\\1.0 sun\\transformer\\transformer_model.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_mask \u001b[39m=\u001b[39m mask\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(src)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(src,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msrc_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[0;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:592\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[39m=\u001b[39mis_causal))\n\u001b[1;32m--> 592\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ff_block(x))\n\u001b[0;32m    594\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:607\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ff_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 607\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(x))))\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = 6  # number of features in the outdoor conditions and the indoor measurements\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # the number of features in the output\n",
    "num_epochs = 2\n",
    "num_layers = 1\n",
    "num_mixtures = 5\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "Fold = 0\n",
    "Predictions = np.zeros((6, 4794, 2))\n",
    "Test = np.zeros((6, 4794, 2))\n",
    "kf = KFold(n_splits=12)\n",
    "\n",
    "\n",
    "num_samples = all_new_features.shape[0]\n",
    "\n",
    "\n",
    " \n",
    "all_new_features=all_new_features[0:12, :, :].astype(np.float32)\n",
    "all_new_featurest= torch.from_numpy(all_new_features).float().to(device)\n",
    "outdoor_panels_power_averaget= torch.from_numpy(outdoor_panels_power_average[0:12, :, :]).float().to(device)\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 12\n",
    "\n",
    "# Initialize lists to store the average MSE for each fold\n",
    "average_mse_scores1 = []\n",
    "average_mse_scores2 = []\n",
    "fold = 0\n",
    "y_pred1=np.zeros(( 1, 4794))\n",
    "y_pred2=np.zeros(( 1, 4794))\n",
    "# Initialize arrays to store the predictions and test data for each fold\n",
    "predictions1 = np.zeros((12, 1, 4794))\n",
    "test1= np.zeros((12, 1, 4794))\n",
    "predictions2 = np.zeros((12, 1, 4794))\n",
    "test2 = np.zeros((12, 1, 4794))\n",
    "# Perform k-fold cross-validation\n",
    " \n",
    "for temp in range(6):\n",
    "    test_index=[2*temp,2*temp+1]\n",
    "    train_index = [i for i in range(num_samples) if i not in test_index]\n",
    "    # Split the data into training and test sets for the current fold\n",
    "    X_train, X_test = all_new_featurest[train_index], all_new_featurest[test_index]\n",
    "    y_train, y_test = outdoor_panels_power_averaget[train_index], outdoor_panels_power_averaget[test_index]\n",
    "    model = TransAm(feature_size=4, num_layers=15, dropout=0.5).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # Training loop\n",
    "    for epoch in range(1):\n",
    "        model.train()  # Set the model in training mode\n",
    "        outputs = model(X_train)\n",
    "        outputs = outputs.view(X_train.shape[0], 4794, 1)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()  # Set the model in evaluation mode for testing\n",
    "    with torch.no_grad():\n",
    "        sample_input = X_test\n",
    "        predictions = model(sample_input)\n",
    "        predictions_temp = predictions\n",
    "        predictions_temp = predictions_temp[:,:,0].T\n",
    "        Predictions[Fold] = predictions_temp.cpu().numpy()\n",
    "        y_test_temp = y_test\n",
    "\n",
    "        y_test_temp = y_test_temp[:,:,0].T\n",
    "        Test[Fold] = y_test_temp.cpu().numpy()\n",
    "\n",
    "        mse = F.mse_loss(predictions, y_test)\n",
    "        print(\"Mean Squared Error (MSE):\", mse.item())\n",
    "        y_test_temp = y_test_temp.cpu()\n",
    "        predictions_temp = predictions_temp.cpu()\n",
    "#         Plotting the predictions against the actual values\n",
    "        x = range(y_test_temp.T.shape[1])\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        for i, axs in enumerate(axs.flat):\n",
    "            print(i)\n",
    "            axs.plot(x, y_test_temp.T[i].flatten(), label='Actual')\n",
    "            axs.plot(x, predictions_temp.T[i].flatten(), label='Prediction', color='red')\n",
    "            axs.set_xlabel('Time')\n",
    "            axs.set_ylabel('Value')\n",
    "            axs.legend()\n",
    "        fig.suptitle('Predictions vs Actual Values')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        Fold += 1\n",
    "\n",
    "        # If Fold reaches 2, stop the training loop (for demonstration purposes)\n",
    "        if Fold == 6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window function, split data into sequence window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in training and testing, prepared in windowed sequences and pass through GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function for model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to forecast 1 time step from window sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, seqence):\n",
    "    model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "\n",
    "    seq = np.pad(seqence, (0, 3), mode='constant', constant_values=(0, 0))\n",
    "    seq = create_inout_sequences(seq, input_window)\n",
    "    seq = seq[:-output_window].to(device)\n",
    "\n",
    "    seq, _ = get_batch(seq, 0, 1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, output_window):            \n",
    "            output = model(seq[-output_window:])                        \n",
    "            seq = torch.cat((seq, output[-1:]))\n",
    "\n",
    "    seq = seq.cpu().view(-1).numpy()\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to forecast entire sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_seq(model, sequences):\n",
    "    \"\"\"Sequences data has to been windowed and passed through device\"\"\"\n",
    "    start_timer = time.time()\n",
    "    model.eval() \n",
    "    forecast_seq = torch.Tensor(0)    \n",
    "    actual = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sequences) - 1):\n",
    "            data, target = get_batch(sequences, i, 1)\n",
    "            output = model(data)            \n",
    "            forecast_seq = torch.cat((forecast_seq, output[-1].view(-1).cpu()), 0)\n",
    "            actual = torch.cat((actual, target[-1].view(-1).cpu()), 0)\n",
    "    timed = time.time()-start_timer\n",
    "    print(f\"{timed} sec\")\n",
    "\n",
    "    return forecast_seq, actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\benal\\indoor-outdoor_stability_prediction\\Old _data\\1.0 sun\\transformer\\transformer_model.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_data, val_data \u001b[39m=\u001b[39m get_data(logreturn, \u001b[39m0.6\u001b[39m) \u001b[39m# 60% train, 40% test split\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benal/indoor-outdoor_stability_prediction/Old%20_data/1.0%20sun/transformer/transformer_model.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m TransAm()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_data(logreturn, 0.6) # 60% train, 40% test split\n",
    "model = TransAm().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # Loss function\n",
    "lr = 0.00005 # learning rate\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "epochs =  5 # Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data)\n",
    "    \n",
    "    if(epoch % epochs == 0): # Valid model after last training epoch\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        print('-' * 80)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss: {:5.7f}'.format(epoch, (time.time() - epoch_start_time), val_loss))\n",
    "        print('-' * 80)\n",
    "\n",
    "    else:   \n",
    "        print('-' * 80)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time)))\n",
    "        print('-' * 80)\n",
    "\n",
    "    scheduler.step() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result, truth = forecast_seq(model, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot forecasted sequence vs actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model on Boeing stock from the same time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model on JPMorgan stock from the same time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Transformer model is able to train on a longer sequence compared to LSTM models (5 vs 10)  \n",
    "Transformer model also trained faster, given the longer sequence length, and took lesser epochs to train the model\n",
    "\n",
    "### Data Transforming and Normalising\n",
    "Advantages of using cumulative sum of log returns compared to min-max scaler in normalising stocks prices,  \n",
    "create better standardisation across stocks  \n",
    "Training data augmentation allowed model to be trained on wider data points, resulting in the model generalising well across unseen data from test 2 and test 3  \n",
    "Previous testing without data augmentation had models underperforming with higher loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
